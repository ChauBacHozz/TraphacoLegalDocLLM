{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "vector_db_path = \"vectorstores/db_faiss\"\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = GPT4AllEmbeddings(model_file=\"models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "\n",
    "# Load the FAISS vector database with the embedding model\n",
    "db = FAISS.load_local(folder_path=vector_db_path, embeddings=embedding_model, allow_dangerous_deserialization = True)\n",
    "\n",
    "# Perform a similarity search\n",
    "def search_query(query: str, k: int = 30):\n",
    "    \"\"\"\n",
    "    Perform a similarity search on the vector database.\n",
    "    \n",
    "    :param query: The query string.\n",
    "    :param k: The number of top results to return.\n",
    "    :return: The top results as a list of strings.\n",
    "    \"\"\"\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    return [result.page_content for result in results]\n",
    "\n",
    "# Example query\n",
    "# query = \"Thủ tục cấp chứng chỉ hành nghề dược gồm các nội dung gì?\"\n",
    "# top_results = search_query(query, k=30)\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nTop Results:\")\n",
    "# for result in top_results:\n",
    "#     print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import CTransformers\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain import HuggingFacePipeline\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "# # from ctransformers import AutoConfig\n",
    "# # from ctransformers import AutoModelForCausalLM\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch \n",
    "\n",
    "# model_file = \"models/vinallama-7b-chat_q5_0.gguf\"\n",
    "\n",
    "# # Cau hinh\n",
    "# # config = AutoConfig.from_pretrained(model_file)\n",
    "# # config.config.max_new_tokens = 2000\n",
    "# # config.config.context_length = 6000\n",
    "\n",
    "# # Load LLM\n",
    "# def load_llm(model_file):\n",
    "#     llm = CTransformers(\n",
    "#         model = model_file,\n",
    "#         model_type=\"llama\",\n",
    "#         torch_dtype=\"auto\",\n",
    "#         device_map=\"auto\",\n",
    "#         # load_in_8bit=True,\n",
    "#         config={'max_new_tokens': 2000,\n",
    "#                 'temperature': 0.01,\n",
    "#                 'context_length': 6000,\n",
    "#                 }\n",
    "#     )\n",
    "#     return llm\n",
    "\n",
    "# # Tao prompt template\n",
    "# def creat_prompt(template):\n",
    "#     prompt = PromptTemplate(template = template, input_variables=[\"context\", \"question\"])\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "# # Tao simple chain\n",
    "# def create_qa_chain(prompt, llm, db):\n",
    "#     llm_chain = RetrievalQA.from_chain_type(\n",
    "#         llm = llm,\n",
    "#         chain_type= \"stuff\",\n",
    "#         retriever = db.as_retriever(search_kwargs = {\"k\":2}, max_tokens_limit=1024),\n",
    "#         return_source_documents = False,\n",
    "#         chain_type_kwargs= {'prompt': prompt}\n",
    "\n",
    "#     )\n",
    "#     return llm_chain\n",
    "\n",
    "# # Read tu VectorDB\n",
    "# def read_vectors_db():\n",
    "#     # Embeding\n",
    "#     embedding_model = GPT4AllEmbeddings(model_file=\"models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "#     db = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization = True)\n",
    "#     return db\n",
    "\n",
    "# # Define a question-answering pipeline using the model and tokenizer\n",
    "\n",
    "# # question_answerer = pipeline(\n",
    "# #     \"question-answering\", \n",
    "# #     model=model_file, \n",
    "# #     tokenizer=tokenizer,\n",
    "# #     return_tensors='pt'\n",
    "# # )\n",
    "\n",
    "# # Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# # with additional model-specific arguments (temperature and max_length)\n",
    "# # llm = HuggingFacePipeline(\n",
    "# #     pipeline=question_answerer,\n",
    "# #     model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    "# # )\n",
    "# llm = load_llm(model_file)\n",
    "# #Tao Prompt\n",
    "# template = \"\"\"<|im_start|>system\\nSử dụng thông tin sau đây để trả lời câu hỏi. Nếu bạn không biết câu trả lời, hãy nói không biết, đừng cố tạo ra câu trả lời, không tóm tắt, diễn đạt lại hoặc diễn giải nội dung—trả về văn bản gốc chính xác như trong nguồn.\\n\n",
    "#     {context}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\"\"\"\n",
    "# prompt = creat_prompt(template)\n",
    "\n",
    "# llm_chain = create_qa_chain(prompt, llm, db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phongnd/anaconda3/envs/llm2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import torch\n",
    "\n",
    "# Define the path for the model\n",
    "model_file = \"AITeamVN/Vi-Qwen2-7B-RAG\"\n",
    "\n",
    "# Define the FAISS vector store path\n",
    "vector_db_path = \"vectorstores/db_faiss\"\n",
    "\n",
    "# Load the Hugging Face model and tokenizer\n",
    "def load_huggingface_model(model_file):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_file, torch_dtype=torch.float16, device_map=\"auto\", load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_file)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Read the vector database (FAISS)\n",
    "def read_vectors_db():\n",
    "    embedding_model = GPT4AllEmbeddings(model_file=\"models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "    db = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    return db\n",
    "\n",
    "# Perform similarity search on the vector database\n",
    "def search_vector_db(query, k=2):\n",
    "    db = read_vectors_db()\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    return [result.page_content for result in results]\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = load_huggingface_model(model_file)\n",
    "\n",
    "\n",
    "system_prompt = \"Bạn là một trợ lí Tiếng Việt nhiệt tình và trung thực. Hãy luôn trả lời một cách hữu ích nhất có thể.\"\n",
    "template = '''Chú ý các yêu cầu sau:\n",
    "- Câu trả lời phải chính xác và đầy đủ nếu ngữ cảnh có câu trả lời. \n",
    "- Chỉ sử dụng các thông tin có trong ngữ cảnh được cung cấp.\n",
    "- Chỉ cần từ chối trả lời và không suy luận gì thêm nếu ngữ cảnh không có câu trả lời.\n",
    "- Nếu nhiều nội dung được lấy từ cùng 1 khoản trong tài liệu đã cho, trả về toàn bộ nội dung trong khoản đó một cách chính xác nhất, không thực hiện tóm tắt lại.\n",
    "Hãy trả lời câu hỏi dựa trên ngữ cảnh:\n",
    "### Ngữ cảnh :\n",
    "{context}\n",
    "\n",
    "### Câu hỏi :\n",
    "{question}\n",
    "\n",
    "### Trả lời :'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "question = '''Hồ sơ đề nghị cấp chứng chỉ hành nghề  dược đối với người nước ngoài gồm những nội dung nào, trả lời một cách chi tiết và đầy đủ nhất'''\n",
    "context_list = search_vector_db(question, k = 50)\n",
    "context = \"\\n\".join(context_list)\n",
    "conversation = [{\"role\": \"system\", \"content\": system_prompt }]\n",
    "conversation.append({\"role\": \"user\", \"content\": template.format(context = context, question = question)})\n",
    "text = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True)\n",
    "model_inputs = tokenizer(text,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    # temperature = 0.1,\n",
    "    #top_p=0.95,\n",
    "    #top_k=40,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dựa trên ngữ cảnh cung cấp, hồ sơ đề nghị cấp Chứng chỉ hành nghề dược đối với người nước ngoài bao gồm các nội dung sau:\n",
      "\n",
      "1. **Đơn đề nghị cấp Chứng chỉ hành nghề dược**:\n",
      "   - Thực hiện theo Mẫu số 02 tại Phụ lục I ban hành kèm theo Nghị định này.\n",
      "   - 02 ảnh chân dung cỡ 4 cm x 6 cm của người đề nghị cấp Chứng chỉ hành nghề dược chụp trên nền trắng trong thời gian không quá 06 tháng.\n",
      "\n",
      "2. **Bản chính hoặc bản sao có chứng thực Giấy chứng nhận sức khỏe**:\n",
      "   - Do cơ sở khám bệnh, chữa bệnh cấp theo quy định tại Luật khám bệnh, chữa bệnh.\n",
      "\n",
      "3. **Bản chính hoặc bản sao chứng thực giấy xác nhận kết quả thi**:\n",
      "   - Do cơ sở tổ chức thi quy định tại khoản 2 Điều 28 của Nghị định này cấp đối với trường hợp Chứng chỉ hành nghề dược cấp theo hình thức\n",
      "thi.\n",
      "\n",
      "4. **Giấy tờ chứng minh về việc đáp ứng yêu cầu về sử dụng ngôn ngữ**:\n",
      "   - Đối với người nước ngoài, người Việt Nam định cư ở nước ngoài đề nghị cấp Chứng chỉ hành nghề dược theo hình thức xét hồ sơ, phải có\n",
      "các tài liệu chứng minh về việc đáp ứng yêu cầu về sử dụng ngôn ngữ theo quy định tại khoản 2 Điều 14 của Luật dược.\n",
      "\n",
      "5. **Giấy tờ do cơ quan có thẩm quyền nước ngoài cấp**:\n",
      "   - Phải được hợp pháp hóa lãnh sự theo quy định.\n",
      "   - Các giấy tờ này phải có bản dịch sang tiếng Việt và được công chứng theo quy định.\n",
      "\n",
      "Như vậy, hồ sơ đề nghị cấp Chứng chỉ hành nghề dược đối với người nước ngoài phải bao gồm đầy đủ các nội dung trên để đảm bảo tính hợp lệ và\n",
      "chính xác.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Example text (you can replace this with your query result)\n",
    "text = response\n",
    "\n",
    "# Function to wrap text\n",
    "def wrap_text(text, width=80):\n",
    "    wrapper = textwrap.TextWrapper(width=width, replace_whitespace=False)\n",
    "    wrapped_text = \"\\n\".join([wrapper.fill(line) for line in text.splitlines()])\n",
    "    return wrapped_text\n",
    "\n",
    "# Wrap and print the text\n",
    "wrapped_text = wrap_text(text, width=140)\n",
    "print(wrapped_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
